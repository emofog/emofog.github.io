{"config":{"lang":["en"],"separator":"[\\s\\u200b\\u3000\\-\u3001\u3002\uff0c\uff0e\uff1f\uff01\uff1b]+","pipeline":["stemmer"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"\ud83c\udfe0 \u4e3b\u9875","text":""},{"location":"#_1","title":"\ud83c\udfe0 (\u25d5\u203f\u25d5)","text":"<p>\ud83d\udcd3 \u535a\u5ba2 \u00a0\u00a0\u00a0/\u00a0\u00a0\u00a0 \ud83d\udd17 \u670b\u53cb\u4eec</p>"},{"location":"links/","title":"\ud83d\udd17 \u670b\u53cb\u4eec","text":"secreu"},{"location":"note/","title":"Index","text":""},{"location":"note/#_1","title":"\u65e5\u95f4 / \u591c\u95f4","text":"<code>default</code> <code>slate</code>"},{"location":"note/#_2","title":"\u4e3b\u8272","text":"red pink purple indigo blue cyan teal green lime orange brown grey black white"},{"location":"paper/beat/","title":"BEAT: A Large-Scale Semantic and Emotional Multi-Modal Dataset for Conversational Gestures Synthesis","text":"<p>\u968f\u7740\u5143\u5b87\u5b99\u7684\u706b\u7206\u4ee5\u53ca\u6570\u5b57\u4eba\u5efa\u6a21\u6280\u672f\u7684\u5546\u4e1a\u5316\uff0cAI \u6570\u5b57\u4eba\u9a71\u52a8\u7b97\u6cd5\uff0c\u4f5c\u4e3a\u6570\u5b57\u4eba\u52a8\u753b\u6280\u672f\u94fe\u7684\u4e0b\u4e00\u5173\u952e\u73af\u8282\uff0c\u83b7\u5f97\u4e86\u5b66\u754c\u548c\u5de5\u4e1a\u754c\u8d8a\u6765\u8d8a\u5e7f\u6cdb\u7684\u5174\u8da3\u548c\u5173\u6ce8\u3002\u5176\u4e2d\u8c08\u8bdd\u52a8\u4f5c\u751f\u6210\uff08\u7531\u58f0\u97f3\u7b49\u63a7\u5236\u4fe1\u53f7\u751f\u6210\u80a2\u4f53\u548c\u624b\u90e8\u52a8\u4f5c\uff09\u7531\u4e8e\u53ef\u4ee5\u964d\u4f4e VR Chat\u3001\u865a\u62df\u76f4\u64ad\u3001\u6e38\u620f NPC \u7b49\u573a\u666f\u4e0b\u7684\u9a71\u52a8\u6210\u672c\uff0c\u5728\u8fd1\u4e24\u5e74\u6210\u4e3a\u7814\u7a76\u70ed\u70b9\u3002</p> <p>\u7136\u800c\uff0c\u7531\u4e8e\u7f3a\u4e4f\u5f00\u6e90\u6570\u636e\uff0c\u73b0\u6709\u7684\u6a21\u578b\u5f80\u5f80\u5728\u7531\u59ff\u6001\u68c0\u6d4b\u7b97\u6cd5\u63d0\u4f9b\u7684\u4f2a\u6807\u7b7e\u6570\u636e\u96c6\u6216\u8005\u5355\u4e2a\u8bf4\u8bdd\u4eba\u7684\u5c0f\u89c4\u6a21\u52a8\u6355\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002\u7531\u4e8e\u6570\u636e\u91cf\u3001\u6570\u636e\u6807\u6ce8\u7684\u7f3a\u4e4f\u548c\u6570\u636e\u8d28\u91cf\u7684\u9650\u5236\uff0c\u73b0\u6709\u7684\u7b97\u6cd5\u5f88\u96be\u751f\u6210\u4e2a\u6027\u5316\u3001\u9ad8\u624b\u90e8\u8d28\u91cf\u3001\u60c5\u611f\u76f8\u5173\u3001\u52a8\u4f5c-\u8bed\u4e49\u76f8\u5173\u7684\u52a8\u4f5c\u3002</p> <p>\u9488\u5bf9\u4e0a\u8ff0\u95ee\u9898\uff0c\u534e\u4e3a\u4e1c\u4eac\u7814\u7a76\u6240 - Digital Human Lab \u4e0e\u4e1c\u4eac\u5927\u5b66\u7b49\u5408\u4f5c\u8fdb\u884c\u4e86\u7814\u7a76\uff0c\u63d0\u51fa\u4e86\u76ee\u524d\u4e3a\u6b62\u6700\u5927\u89c4\u6a21\u7684\u6570\u5b57\u4eba\u591a\u6a21\u6001\u6570\u636e\u96c6\uff1aBEAT\uff08Body-Expression-Audio-Text\uff09\uff0c\u7531 76 \u5c0f\u65f6\u52a8\u6355\u8bbe\u5907\u91c7\u96c6\u7684\u8c08\u8bdd\u6570\u636e\u548c\u8bed\u4e49-\u60c5\u611f\u6807\u6ce8\u7ec4\u6210\u3002\u539f\u59cb\u6570\u636e\u5305\u542b\uff1a</p> <ul> <li>\u80a2\u4f53\u548c\u624b\u90e8\u52a8\u6355\u6570\u636e  </li> <li>AR Kit \u6807\u51c6 52 \u7ef4\u9762\u90e8 blendshape \u6743\u91cd  </li> <li>\u97f3\u9891\u4e0e\u6587\u672c  </li> </ul> <p>\u6807\u6ce8\u6570\u636e\u5305\u542b\uff1a</p> <ul> <li>8 \u7c7b\u60c5\u611f\u5206\u7c7b\u6807\u7b7e  </li> <li>\u52a8\u4f5c\u7c7b\u578b\u5206\u7c7b  </li> <li>\u8bed\u4e49\u76f8\u5173\u5ea6\u6253\u5206  </li> </ul> <p>\u5728 BEAT \u7684\u57fa\u7840\u4e0a\u63d0\u51fa\u7684\u65b0\u57fa\u7ebf\u6a21\u578b CaMN (Cascade-Motion-Network) \u91c7\u53d6\u7ea7\u8054\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\uff0c\u7531 BEAT \u4e2d\u5176\u4f59\u4e09\u79cd\u6a21\u6001\u548c\u6807\u6ce8\u4f5c\u4e3a\u8f93\u5165\uff0c\u5728\u52a8\u4f5c\u751f\u6210\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709 SoTA (state-of-the-art) \u7b97\u6cd5\u3002</p> <p>\u8bba\u6587\u300aBEAT: A Large-Scale Semantic and Emotional Multi-Modal Dataset for Conversational Gestures Synthesis\u300b\u5df2\u4e8e ECCV2022 \u4e0a\u53d1\u8868\uff0c\u6570\u636e\u96c6\u5df2\u7ecf\u5f00\u6e90\u3002</p> <p>\u4f5c\u8005: Haiyang Liu, Zihao Zhu, Naoya Iwamoto, Yichen Peng, Zhengqing Li, You Zhou, Elif Bozkurt, Bo Zheng  </p> <p>\u5355\u4f4d\uff1aDigital Human Lab - \u534e\u4e3a\u4e1c\u4eac\u7814\u7a76\u6240\uff0c\u4e1c\u4eac\u5927\u5b66\uff0c\u5e86\u5e94\u5927\u5b66\uff0c\u5317\u9646\u5148\u7aef\u79d1\u6280\u5927\u5b66  </p> <p>\u8bba\u6587\u5730\u5740\uff1ahttps://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670605.pdf </p> <p>\u9879\u76ee\u4e3b\u9875\uff1ahttps://pantomatrix.github.io/BEAT/ </p> <p>\u6570\u636e\u96c6\u4e3b\u9875\uff1ahttps://pantomatrix.github.io/BEAT-Dataset/ </p> <p>\u89c6\u9891\u7ed3\u679c\uff1ahttps://www.youtube.com/watch?v=F6nXVTUY0KQ</p>"}]}